{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5870b6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, SimpleRNN\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import os\n",
    "import csv\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def create_dataset(dataset, look_back=1):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back), 0]\n",
    "        X.append(a)\n",
    "        Y.append(dataset[i + look_back, 0])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "folder_path = 'SeaData\\\\csv\\\\d10'\n",
    "date = 'd10'\n",
    "\n",
    "with open(f\"Statistics for RNN {date}.csv\", \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([f'Statistics for RNN {date}'])\n",
    "    \n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        print(f'Processing file: {file_path}')\n",
    "        tag = ''\n",
    "        if 'dbiastg' in file_name:\n",
    "            tag = 'dbiastg'\n",
    "        else:\n",
    "            tag = 'biastg'   \n",
    "        writer.writerow([file_name])       \n",
    "\n",
    "        data = pd.read_csv(file_path, index_col='date', parse_dates=True)\n",
    "\n",
    "        # Prepare the data\n",
    "        data = data[tag]\n",
    "        data = data.asfreq('B').fillna(method='ffill')  # Fill missing values\n",
    "\n",
    "        # Normalize the dataset\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        data = scaler.fit_transform(data.values.reshape(-1, 1))\n",
    "\n",
    "        # Split the data into training and testing sets\n",
    "        train_size = int(len(data) * 0.8)\n",
    "        test_size = len(data) - train_size\n",
    "        train_data, test_data = data[0:train_size,:], data[train_size:len(data),:]\n",
    "\n",
    "        # Reshape the data into X=t and Y=t+1\n",
    "        look_back = 1\n",
    "        X_train, Y_train = create_dataset(train_data, look_back)\n",
    "        X_test, Y_test = create_dataset(test_data, look_back)\n",
    "\n",
    "        # Reshape the input to be [samples, time steps, features]\n",
    "        X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "        X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "        # Create and fit the RNN network\n",
    "        model = Sequential()\n",
    "        model.add(SimpleRNN(4, input_shape=(1, look_back)))\n",
    "        model.add(Dense(1))\n",
    "        model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "        model.fit(X_train, Y_train, epochs=100, batch_size=1, verbose=2)\n",
    "\n",
    "        # Make predictions\n",
    "        train_predict = model.predict(X_train)\n",
    "        test_predict = model.predict(X_test)\n",
    "\n",
    "        # Invert predictions\n",
    "        train_predict = scaler.inverse_transform(train_predict)\n",
    "        Y_train = scaler.inverse_transform([Y_train])\n",
    "        test_predict = scaler.inverse_transform(test_predict)\n",
    "        Y_test = scaler.inverse_transform([Y_test])\n",
    "\n",
    "        # Evaluate the model\n",
    "        mae = mean_absolute_error(Y_test[0], test_predict[:,0])\n",
    "        mse = mean_squared_error(Y_test[0], test_predict[:,0])\n",
    "        rmse = np.sqrt(mse)\n",
    "        mbe = np.mean(Y_test[0] - test_predict[:,0])\n",
    "        mape = mean_absolute_percentage_error(Y_test[0], test_predict[:,0])\n",
    "\n",
    "\n",
    "        print(f\"RMSE: {rmse:.2f}\")\n",
    "        writer.writerow(['RMSE',rmse])\n",
    "        print(f\"MAE: {mae:.2f}\")\n",
    "        writer.writerow(['MAE',mae])\n",
    "        print(f\"MBE: {mbe:.2f}\")\n",
    "        writer.writerow(['MBE',mbe])\n",
    "        print(f\"MAPE: {mape:.2f}%\")\n",
    "        writer.writerow(['MAPE',mape])\n",
    "\n",
    "        # Plot the results\n",
    "        plt.figure(figsize=(14, 6))\n",
    "        plt.plot(data, label='Observed')\n",
    "        plt.plot(np.concatenate((train_predict, test_predict)),color='r', label='Predicted')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel(tag)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(14, 6))\n",
    "        plt.plot(train_data, label='Train Data', color='blue')\n",
    "        plt.plot(np.arange(len(train_data), len(train_data) + len(test_data)), test_data, label='Test Data', color='green')\n",
    "        plt.plot(np.concatenate((train_predict, test_predict)), color='red', linestyle='--', label='Predicted')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel(tag)\n",
    "        plt.legend()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a433386",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, GRU\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import os\n",
    "import csv\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def create_dataset(dataset, look_back=1):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back), 0]\n",
    "        X.append(a)\n",
    "        Y.append(dataset[i + look_back, 0])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "folder_path = 'SeaData\\\\csv\\\\d10'\n",
    "date = 'd10'\n",
    "\n",
    "with open(f\"Statistics for RNN {date}.csv\", \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([f'Statistics for RNN {date}'])\n",
    "\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        print(f'Processing file: {file_path}')\n",
    "        tag = ''\n",
    "        if 'dbiastg' in file_name:\n",
    "            tag = 'dbiastg'\n",
    "        else:\n",
    "            tag = 'biastg'\n",
    "        writer.writerow([file_name])\n",
    "\n",
    "        data = pd.read_csv(file_path, index_col='date', parse_dates=True)\n",
    "\n",
    "        # Prepare the data\n",
    "        data = data[tag]\n",
    "        data = data.asfreq('B').fillna(method='ffill')  # Fill missing values\n",
    "\n",
    "        # Normalize the dataset\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        data = scaler.fit_transform(data.values.reshape(-1, 1))\n",
    "\n",
    "        # Split the data into training and testing sets\n",
    "        train_size = int(len(data) * 0.8)\n",
    "        test_size = len(data) - train_size\n",
    "        train_data, test_data = data[0:train_size,:], data[train_size:len(data),:]\n",
    "\n",
    "        # Reshape the data into X=t and Y=t+1\n",
    "        look_back = 3  # Increase the look-back window\n",
    "        X_train, Y_train = create_dataset(train_data, look_back)\n",
    "        X_test, Y_test = create_dataset(test_data, look_back)\n",
    "        # Reshape the input to be [samples, time steps, features]\n",
    "        X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "        X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "        # Create and fit the RNN network\n",
    "        model = Sequential()\n",
    "        model.add(GRU(32, input_shape=(1, look_back), return_sequences=True))  # Use GRU layer with 32 units and return_sequences=True for stacking\n",
    "        model.add(GRU(16))  # Add another GRU layer with 16 units\n",
    "        model.add(Dense(1))\n",
    "        model.compile(loss='mean_squared_error', optimizer='Nadam')  # Use Nadam optimizer\n",
    "        model.fit(X_train, Y_train, epochs=100, batch_size=1, verbose=2)\n",
    "\n",
    "        # Make predictions\n",
    "        train_predict = model.predict(X_train)\n",
    "        test_predict = model.predict(X_test)\n",
    "\n",
    "        # Invert predictions\n",
    "        train_predict = scaler.inverse_transform(train_predict)\n",
    "        Y_train = scaler.inverse_transform([Y_train])\n",
    "        test_predict = scaler.inverse_transform(test_predict)\n",
    "        Y_test = scaler.inverse_transform([Y_test])\n",
    "\n",
    "        # Evaluate the model\n",
    "        mae = mean_absolute_error(Y_test[0], test_predict[:,0])\n",
    "        mse = mean_squared_error(Y_test[0], test_predict[:,0])\n",
    "        rmse = np.sqrt(mse)\n",
    "        mbe = np.mean(Y_test[0] - test_predict[:,0])\n",
    "        mape = mean_absolute_percentage_error(Y_test[0], test_predict[:,0])\n",
    "\n",
    "        print(f\"RMSE: {rmse:.2f}\")\n",
    "        writer.writerow(['RMSE', rmse])\n",
    "        print(f\"MAE: {mae:.2f}\")\n",
    "        writer.writerow(['MAE', mae])\n",
    "        print(f\"MBE: {mbe:.2f}\")\n",
    "        writer.writerow(['MBE', mbe])\n",
    "        print(f\"MAPE: {mape:.2f}%\")\n",
    "        writer.writerow(['MAPE', mape])\n",
    "\n",
    "        # Plot the results\n",
    "        plt.figure(figsize=(14, 6))\n",
    "        plt.plot(data, label='Observed')\n",
    "        plt.plot(np.concatenate((train_predict, test_predict)), color='r', label='Predicted')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel(tag)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(14, 6))\n",
    "        plt.plot(train_data, label='Train Data', color='blue')\n",
    "        plt.plot(np.arange(len(train_data), len(train_data) + len(test_data)), test_data, label='Test Data', color='green')\n",
    "        plt.plot(np.concatenate((train_predict, test_predict)), color='red', linestyle='--', label='Predicted')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel(tag)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
